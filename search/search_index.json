{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Stony Brook University. 700 Health Sciences Drive. SeungminChou@stonybrook.edu.</p> <p>700 Health Sciences Dr Stonybrook, New York 11790 Hi there, this is Seungmin. I am a doctorate student at Stony Brook majoring in Data Science. Before Stony Brook, I received my bachelor\u2019s degree at Chung-Ang University, where I was honorably supervised by professor Changwon Lim.</p> <p>I\u2019m posting some interesting parts of statistics, mathematics and data mining, so feel free to stay here! For now, what interests me is how to build a Bayesian model that is transparent, reliable and yet practical, which does not necessarily rely on Gradient Descent Algorithm. Any idea is welcome!</p> <ul> <li>Kernel Based Learning</li> </ul>"},{"location":"_latexhowto/","title":"latexhowto","text":""},{"location":"_latexhowto/#markdown","title":"Markdown","text":"<p>Text can be bold, italic, ~~strikethrough~~ or <code>keyword</code>.</p> <p>Link to another page.</p> <p>There should be whitespace between paragraphs.</p>"},{"location":"_latexhowto/#header-1","title":"Header 1","text":"<p>This is a normal paragraph following a header. GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.</p>"},{"location":"_latexhowto/#header-2","title":"Header 2","text":"<p>This is a blockquote following a header.</p> <p>When something is important enough, you do it even if the odds are not in your favor.</p>"},{"location":"_latexhowto/#header-3","title":"Header 3","text":"<p>{% highlight js %} // Javascript code with syntax highlighting. var fun = function lang(l) {   dateformat.i18n = require('./lang/' + l)   return true; } {% endhighlight %}</p> <p>{% highlight ruby %}</p>"},{"location":"_latexhowto/#ruby-code-with-syntax-highlighting","title":"Ruby code with syntax highlighting","text":"<p>GitHubPages::Dependencies.gems.each do |gem, version|   s.add_dependency(gem, \"= #{version}\") end {% endhighlight %}</p>"},{"location":"_latexhowto/#header-4","title":"Header 4","text":"<ul> <li>This is an unordered list following a header.</li> <li>This is an unordered list following a header.</li> <li>This is an unordered list following a header.</li> </ul>"},{"location":"_latexhowto/#header-5","title":"Header 5","text":"<ol> <li>This is an ordered list following a header.</li> <li>This is an ordered list following a header.</li> <li>This is an ordered list following a header.</li> </ol>"},{"location":"_latexhowto/#header-6","title":"Header 6","text":"head1 head two three ok good swedish fish nice out of stock good and plenty nice ok good <code>oreos</code> hmm ok good <code>zoute</code> drop yumm"},{"location":"_latexhowto/#theres-a-horizontal-rule-below-this","title":"There's a horizontal rule below this.","text":""},{"location":"_latexhowto/#here-is-an-unordered-list","title":"Here is an unordered list:","text":"<ul> <li>Item foo</li> <li>Item bar</li> <li>Item baz</li> <li>Item zip</li> </ul>"},{"location":"_latexhowto/#and-an-ordered-list","title":"And an ordered list:","text":"<ol> <li>Item one</li> <li>Item two</li> <li>Item three</li> <li>Item four</li> </ol>"},{"location":"_latexhowto/#and-a-nested-list","title":"And a nested list:","text":"<ul> <li>level 1 item</li> <li>level 2 item</li> <li>level 2 item<ul> <li>level 3 item</li> <li>level 3 item</li> </ul> </li> <li>level 1 item</li> <li>level 2 item</li> <li>level 2 item</li> <li>level 2 item</li> <li>level 1 item</li> <li>level 2 item</li> <li>level 2 item</li> <li>level 1 item</li> </ul>"},{"location":"_latexhowto/#wide-image","title":"Wide image","text":""},{"location":"_latexhowto/#definition-lists-can-be-used-with-html-syntax","title":"Definition lists can be used with HTML syntax.","text":"Name Godzilla Born 1952 Birthplace Japan Color Green"},{"location":"about/","title":"About Me","text":""},{"location":"about/#markdown","title":"Markdown","text":"<p>Stony Brook University. 700 Health Sciences Drive. SeungminChou@stonybrook.edu.</p> <p>700 Health Sciences Dr Stonybrook, New York 11790 Hi there, this is Seungmin. I am a doctorate student at Stony Brook majoring in Data Science. Before Stony Brook, I received my bachelor\u2019s degree at Chung-Ang University, where I was supervised by professor Changwon Lim.</p> <p>I\u2019m posting some interesting parts of statistics, mathematics and data science, so feel free to stay here! For now, what interests me is how to build a Bayesian model that is transparent, reliable and yet practical. Any idea is welcome!</p>"},{"location":"research/ci/ci/","title":"Causal Inference","text":""},{"location":"research/ci/ci/#machine-learning-pages","title":"Machine Learning Pages","text":"<ol> <li>Blank Blank</li> </ol>"},{"location":"research/ml/kernel%20based%20learning/","title":"Kernel Based Learning","text":"<p>This post deals with kernel based learning that are largely used nowadays in many neural network models, starting from maximal margin classifier through support vector classifier/machine to how they are employed in feature maps in neural networks.</p>"},{"location":"research/ml/kernel%20based%20learning/#support-vector-machine-review","title":"Support Vector Machine Review","text":"<p>Support vector machine (SVM) was developed in 1990s. It is originally from 'maximal margin classifier', which was further developed into 'support vector classifier', leading to SVM.</p>"},{"location":"research/ml/kernel%20based%20learning/#separating-hyperplane-maximal-margin-classifier","title":"Separating Hyperplane (Maximal Margin Classifier)","text":"<p>Suppose a \\(p\\)-dimensional space, a hyperplane is a flat affine subspace of dimension \\(p-1\\).</p> <p>For example, in two dimensions, a hyperplane is defined by the equation $$  \\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0 $$  Without loss of generality(WLOG), we can extend to the \\(p\\)-dimensional setting: $ \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n = 0 $</p> <p>Suppose there exists a point \\(x_i \\in \\mathbb{R}^p\\) , i.e. \\(x_i = (x_1, x_2, \\cdots, x_n)\\)  then the hyperplane divides the \\(p\\)-dimensional space into two halves s.t. $$ \\begin{aligned} \\text{(1) : } \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n &gt; 0 \\ \\text{(2) : } \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n &lt; 0 \\end{aligned} $$</p> <p>Based on the location of subspace, if we classify (1) as \\(y_i =1\\) and (2) as \\(y_i=-1\\), we may say that:</p> \\[ y_i(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n) &gt; 0, \\forall x\\in\\mathbb{R}^p \\]"},{"location":"research/ml/kernel%20based%20learning/#how-to-decide-classifier-with-lagrange-multiplier","title":"How to decide classifier with Lagrange Multiplier","text":"<p>Alright, now let's say that we have an observation \\(x^\\ast\\) Also, let \\(f(x^\\ast)=\\beta_0 + \\beta_1x^\\ast_1 + \\beta_2x^\\ast_2 + \\cdots + \\beta_nx^\\ast_n\\). Then we can assign \\(x^\\ast\\) into class 1 or to class -1. That is, $$ \\hat{y}^\\ast = \\text{sign}f(x^\\ast) $$</p> <p>You may easily think that if the magnitude of \\(f(x^\\ast)\\) is far from 0, then \\(x^\\ast\\) lies far from the hyperplane, and otherwise \\(x^\\ast\\) lies right beside of the (separating) hyperplane.</p> <p>The smallest distance from such \\(x^\\ast\\) from the hyperplane is called 'margin', so the choice of \\(f(x^\\ast)\\) maximizing the margin is called 'maximal margin classifier'.</p> <p>Maximal margin classifier is at last decided by only a handful of data that are close to the hyperplane but not others, and we say that the data 'support' the maximal margin hyperplane. </p> <p>So what we basically want here is to maximize \\(f(x^\\ast)\\), but we need to be cautious of the choices of \\(\\beta\\)s because \\(\\lbrace\\beta\\rbrace_n=\\lbrace1,1, \\cdots ,1\\rbrace\\) is equivalent to  \\(\\lbrace\\beta\\rbrace_n = \\lbrace2,2,\\cdots,2\\rbrace\\), as they do not affect the sign of \\(f(x^\\ast)\\) but only the magnitude. So we constrain \\(\\beta \\text{ s.t.} \\sum_{j=1}^n\\beta_j^{2}=1\\). That is,</p> \\[ \\begin{aligned} &amp;\\max_{\\beta_0, \\cdots, \\beta_p}M \\\\  &amp;\\text{subject to } \\sum_{j=1}^n\\beta_j^{2}=1 \\\\ &amp;\\text{and } y_i(\\beta_0 + \\beta_1x^\\ast_{i1} + \\beta_2x^\\ast_{i2} + \\cdots + \\beta_px^\\ast_{ip}) \\geq M \\\\  \\end{aligned} \\] <p>Let \\(\\textbf{w} = \\frac{1}{M}(\\beta_1, \\cdots \\beta_p)^T\\) and \\(b=\\frac{\\beta_0}{M}\\). Then since \\(\\bf{w^Tw}=\\frac{1}{M^2}\\) , it suffices to</p> \\[ \\begin{aligned} &amp;\\min_{\\mathbf{w}, b}\\frac{1}{2}\\mathbf{w^T w} \\\\  &amp;\\text{subject to } 1-y_i(\\mathbf{w^T x} + b) \\leq 0, \\forall i = 1,\\cdots,n \\end{aligned} \\] <p>for all data point \\(\\lbrace x_i \\rbrace _{i=1}^n\\). To minimize the Lagrangian Function \\(\\mathcal{L}\\) :</p> \\[ \\begin{aligned} \\mathcal{L} = \\frac{1}{2}\\mathbf{w^T w} + \\sum_{i=1}^{n}\\alpha_i(1-y_i(\\mathbf{w^T x_i} + b)) \\\\  \\alpha_i \\geq 0, \\forall i = 1,\\cdots,n \\end{aligned} \\] \\[ \\begin{aligned} &amp;\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{w}} = \\mathbf{w} +\\sum_{i=1}^{n}\\alpha_i(-y_i)(\\mathbf{x_i})  =0 \\implies \\mathbf{w} = \\sum_{i=1}^{n}\\alpha_i y_i \\mathbf{x}_i\\\\  &amp;\\frac{\\partial}{\\partial b} = \\sum_{i=1}^{n} \\alpha_iy_i = 0 \\end{aligned} \\] <p>From the above equation, you may see that \\(\\mathbf{w} = \\sum_{i=1}^{n}\\alpha_i y_i \\mathbf{x}_i\\). Let's substitute this \\(\\mathbf{w}\\) back to the \\(\\mathcal{L}\\). Then the Lagrangian</p> \\[ \\begin{aligned} \\mathcal{L} = -\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\mathbf{x}^T\\mathbf{x} + \\sum_{i=1}^{n} \\alpha_i \\\\  \\text{subject to } \\sum_{i=1}^n\\alpha_i y_i = 0, \\forall i = 1, \\cdots, n \\end{aligned} \\] <p>However, the maximal margin classifier does not exists if the data overlaps. If an extraordinary point exists in the other side of the data, we cannot find a classifer since it does not exists. Instead, we can give some margin to the maximal margin classifier, and we say them 'support vector classifier'.</p>"},{"location":"research/ml/kernel%20based%20learning/#support-vector-classifier","title":"Support Vector Classifier","text":"<p>To be updated</p>"},{"location":"research/ml/kernel%20based%20learning/#support-vector-machine","title":"Support Vector Machine","text":"<p>To be updated</p>"},{"location":"research/ml/kernel%20based%20learning/#what-is-kernel-based-leraning","title":"What is kernel based leraning?","text":"<p>To be updated</p>"},{"location":"research/ml/kernel%20based%20learning/#acknowledges","title":"Acknowledges","text":"<p>I would like to express my sincere gratitude to Professor Wonkuk Kim at Chung-Ang University, as this post is largely based on insights gained from his Data Mining class.</p>"},{"location":"research/ml/ml/","title":"Overview","text":""},{"location":"research/ml/ml/#machine-learning-pages","title":"Machine Learning Pages","text":"<ol> <li>Kernel Based Learning</li> <li>Blank Blank</li> </ol>"}]}